{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaTKLgE8FKSt",
    "outputId": "f5fff2db-0fba-49b4-ebd2-617a430cb934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 26 14:50:28 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.76                 Driver Version: 551.76         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P0             20W /   95W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-_htQga1wDu",
    "outputId": "e933b886-6282-4826-cdc8-5c4353dbb6dd"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqhYRRAI2LVa",
    "outputId": "88a9b9b2-6b67-4c6d-9264-5325e610df8a"
   },
   "outputs": [],
   "source": [
    "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/e7/45/419aa0b37254f1fd62b45bb63836066c5eb81e37d70940e0491e95167eed/torchvision-0.17.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torchvision-0.17.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (2024.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.17.1-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.2/1.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.1/1.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xo8c9fHOBSUv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N04_Nn8h2jrw",
    "outputId": "54c32f42-87c8-42ac-e0f6-27b8d6d8f751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME: C:\\Users\\User\\Downloads\n"
     ]
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully: C:\\Users\\User\\Downloads\\weights\\sam_vit_h_4b8939.pth\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    # Ensure the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Make sure the directory exists\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # Open the file in binary write mode and save the content to the file\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully: {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'\n",
    "\n",
    "# Now include the filename in the save_path\n",
    "save_path = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "\n",
    "# Download the file\n",
    "download_file(url, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_U2Ze_LP2eoE",
    "outputId": "5f904001-1321-4d25-a5dd-a6ed72be5c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Downloads\\weights\\sam_vit_h_4b8939.pth ; exist: True\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FCQjouZs36GD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PjRZacqh3_UX"
   },
   "outputs": [],
   "source": [
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XceNZgA587rG"
   },
   "outputs": [],
   "source": [
    "mask_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0vFiAca29RIE"
   },
   "outputs": [],
   "source": [
    "default_box = {'x': 132, 'y': 151, 'width': 300, 'height': 300, 'label': ''}\n",
    "\n",
    "box = default_box\n",
    "box = np.array([\n",
    "    box['x'],\n",
    "    box['y'],\n",
    "    box['x'] + box['width'],\n",
    "    box['y'] + box['height']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.path.join(HOME, 'Original/train/')\n",
    "os.makedirs(source_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "jFBCjxQd5BL6",
    "outputId": "1bb01eb3-4e22-4499-b2da-a059c0156f9b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "source_dir = os.path.join(HOME, 'Original/train/')\n",
    "target_root_dir = os.path.join(HOME, 'Original_segmentation/train/')\n",
    "\n",
    "os.makedirs(target_root_dir, exist_ok=True)\n",
    "\n",
    "for folder_name in os.listdir(source_dir):\n",
    "    folder_path = os.path.join(source_dir, folder_name)\n",
    "\n",
    "    target_folder_path = os.path.join(target_root_dir, folder_name)\n",
    "    os.makedirs(target_folder_path, exist_ok=True)\n",
    "\n",
    "    for image_filename in os.listdir(folder_path):\n",
    "        IMAGE_PATH = os.path.join(folder_path, image_filename)\n",
    "        image_bgr = cv2.imread(IMAGE_PATH)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        mask_predictor.set_image(image_rgb)\n",
    "\n",
    "        masks, scores, logits = mask_predictor.predict(\n",
    "            box=box,\n",
    "            multimask_output=True\n",
    "        )\n",
    "\n",
    "        mask = masks[np.argmax(scores)]\n",
    "        mask = mask.astype(bool)\n",
    "        masked_image = np.zeros_like(image_rgb)\n",
    "\n",
    "        for c in range(3):\n",
    "            masked_image[:, :, c] = image_rgb[:, :, c] * mask\n",
    "\n",
    "        y_indices, x_indices = np.where(mask)\n",
    "        x_min, x_max = x_indices.min(), x_indices.max()\n",
    "        y_min, y_max = y_indices.min(), y_indices.max()\n",
    "\n",
    "        cropped_image = masked_image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        cropped_image_path = os.path.join(target_folder_path, image_filename + \"_segment.png\")\n",
    "        cv2.imwrite(cropped_image_path, cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOQMwwJd9uyv"
   },
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator(color=sv.Color.red())\n",
    "mask_annotator = sv.MaskAnnotator(color=sv.Color.red(), color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections(\n",
    "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
    "    mask=masks\n",
    ")\n",
    "detections = detections[detections.area == np.max(detections.area)]\n",
    "\n",
    "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections, skip_label=True)\n",
    "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[source_image, segmented_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CObPnWmEvbo"
   },
   "outputs": [],
   "source": [
    "# helper function that loads an image before adding it to the widget\n",
    "\n",
    "import base64\n",
    "\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSDflqAKExCD"
   },
   "outputs": [],
   "source": [
    "IS_COLAB = True\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "\n",
    "widget = BBoxWidget()\n",
    "widget.image = encode_image(\"/content/DWT_GrayScale_db1_1%_100.bmp\")\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-EBb4EfEzYS"
   },
   "outputs": [],
   "source": [
    "widget.bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcMYsieUHYD8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYx7Vg__Zg9c",
    "outputId": "e694cb94-b13c-4dab-9cea-46d96bdb3775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\user\\anaconda3\\lib\\site-packages (4.38.2)\n",
      "Collecting transformers[torch]\n",
      "  Obtaining dependency information for transformers[torch] from https://files.pythonhosted.org/packages/0a/fd/280f4385e76f3c1890efc15fa93f7206134fefad6351397e1bfab6d0d0de/transformers-4.39.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/134.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 134.8/134.8 kB 25.4 kB/s eta 0:00:00\n",
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (2.2.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=12.0.0 from https://files.pythonhosted.org/packages/96/2f/0092154f3e1ebbc814de1f8a9075543d77a7ecc691fbad407df174799abe/pyarrow-15.0.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-15.0.2-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<=2024.2.0,>=2023.1.0 from https://files.pythonhosted.org/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from evaluate) (0.13.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from responses<0.19->evaluate) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "   ---------------------------------------- 0.0/510.5 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/510.5 kB 2.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 112.6/510.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 256.0/510.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 481.3/510.5 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 510.5/510.5 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.1/84.1 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading pyarrow-15.0.2-cp311-cp311-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.8 MB 5.9 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.7/24.8 MB 7.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.1/24.8 MB 8.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.5/24.8 MB 7.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.1/24.8 MB 8.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.7/24.8 MB 9.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.5/24.8 MB 10.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.2/24.8 MB 10.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.9/24.8 MB 11.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.8/24.8 MB 12.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 6.9/24.8 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.7/24.8 MB 13.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.7/24.8 MB 13.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.8/24.8 MB 14.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.9/24.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.7/24.8 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.9/24.8 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.8/24.8 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.0/24.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.0/24.8 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.1/24.8 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.2/24.8 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.9/24.8 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.0/24.8 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.1/24.8 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.1/24.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.8 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/24.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
      "   ---------------------------------------- 0.0/8.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/8.8 MB 15.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.8/8.8 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.4/8.8 MB 19.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.0/8.8 MB 16.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.8/8.8 MB 16.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.7/8.8 MB 16.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.8/8.8 MB 17.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.9/8.8 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.7/8.8 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.8 MB 18.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.8/8.8 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 0.0/170.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 170.9/170.9 kB ? eta 0:00:00\n",
      "Installing collected packages: pyarrow-hotfix, pyarrow, fsspec, transformers, datasets, evaluate\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.0\n",
      "    Uninstalling fsspec-2024.3.0:\n",
      "      Successfully uninstalled fsspec-2024.3.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.12.0\n",
      "    Uninstalling datasets-2.12.0:\n",
      "      Successfully uninstalled datasets-2.12.0\n",
      "Successfully installed datasets-2.18.0 evaluate-0.4.1 fsspec-2024.2.0 pyarrow-15.0.2 pyarrow-hotfix-0.6 transformers-4.39.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2024.2.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U datasets transformers[torch] evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "55763c31379f41b9b53a5be8c74ceac6",
      "060ea8f370ca4d888dd18067708978bb",
      "de8fe107607d4ab5aff666ed07b5f284",
      "8f567b81eef24f92af7408f9393b1dc1",
      "7d897629b95645c691993ed5d393bf92",
      "13e04241e87c40fda7f08a993d747cc6",
      "b9e0354b90ce49979a398a9336e3a618",
      "8d0e76b014bd4b4cbfeb1b8ece201fdb",
      "ba12d8106db342e6ab6110c7f2a86c39",
      "44c3ac49da7e494286d58374014c1aeb",
      "6ba0fad1ad654236bb3239479128d548",
      "8dd453fad05142bca326f37bd5a24524",
      "f5e75668172747c88424b5874d877438",
      "69fbb29d495941fc9e64cacf0348c748",
      "ff31c0188cfd4c939ba0c7f3c10ad2de",
      "6d35b8cf8f6c43f5b6692ca64dd153d0",
      "35201ef0f08148078a4224a6ef8cfef5",
      "3bbd47b7ec51494c8fc47df2df1a3058",
      "c30f554a3bc0427fb56823bf5cdc8e49",
      "857695d74937411bba8b5e2f597661e0",
      "49b3e19831514e6f911dcd3ae72cc6d5",
      "8fbc24848bba4f5f9a872ff48e265876",
      "1b27e29edb084f13910c127c93ed4ad1",
      "3afb76100d464638a76e63e536dad0d0",
      "52a727a6f12a4ca6829f7b09c8cff320",
      "31f00590a29a4778afbefbf7cbbf407b",
      "c56523b4afca4fb698116ccca14b1d07",
      "ae4cfb01869a4453b6f8c89afb505d22",
      "638d74ea76c9467eab3ccd80b7de57c0",
      "2ce56fc9d32d499888cb24fc3f4322bb",
      "f10feda8b1a04693a0527a97893cbd94",
      "d4d0e4b3db0b47868a96b40219f065e9"
     ]
    },
    "id": "pXhQeWaFZq09",
    "outputId": "04e9d0e4-b2bf-4f08-8301-87409224fe1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ff3636ccff430c800c8621d111ae59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSMmYbTzmKi1",
    "outputId": "d2457bcb-a6e7-4e45-d62d-bb27ec2b4682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been reorganized.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/content/Class Labels of Dataset 2.csv\"\n",
    "labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "source_dir = '/content/'\n",
    "target_base_dir = '/content/train/'\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "    class_dir = os.path.join(target_base_dir, str(row['class']))\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "    source_path = os.path.join(source_dir, f\"DWT_RGB_db1_1%_{str(row['image ID']).zfill(3)}_segment.png\")\n",
    "    target_path = os.path.join(class_dir, f\"DWT_RGB_db1_1%_{str(row['image ID']).zfill(3)}_segment.png\")\n",
    "\n",
    "    shutil.move(source_path, target_path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "dd9fe342e37848de8723ba86ecf5e0cd",
      "aaac7ff8ef764ca78ac76a8a8a71a37b",
      "534ad18a01a94923b07ec259c5ca7c2d",
      "7bae3a510ff54ec2abe72ffbc5c9b67f",
      "d6cddd4c169b447f86e2d57fd270c292",
      "ff8ea62d7760493ea13d009fe794825f",
      "bf336b47dbe5428f9dbc449a9940d1a9",
      "cee83914ab28492baf873dbda4f63eb5",
      "77a119f622d04e339caeec97a1885d27",
      "3a83baf774e242028609944efd2e73e4",
      "83548925c2b04e84af67f7895dc192be",
      "fbd702f2afd546bfbe12e6a006ced860",
      "df9cf85bbd9f45e69e7bc70e71a2c6ce",
      "19921e9686af435f8404359ddefefd8f",
      "867446f48ffc4108b4d9b2ebf5ae2310",
      "369bdd0998334d75a46817fe8c1b2bed",
      "13bf2bd327194ff89ce1e26480a25bb0",
      "a5f1ba99048d4c7aabcd9699d379578f",
      "1a0ad8e7e3d944339da69e17eb783495",
      "2aaf8ddebe1149b79a867219b6d84a54",
      "e20347e3927d4cda827514d751329121",
      "461ccb92e30b4134ba3bc90ba531f414"
     ]
    },
    "id": "UeP5dNiQmXjV",
    "outputId": "172d3042-d328-4465-b566-6b8fcf499180"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d00f3b0260472287830d83a32050c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/16633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc88edbe828438c8ea7500601d76512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 16633\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "data_dir = os.path.join(HOME, 'Original_segmentation/train/')\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=data_dir)\n",
    "wbc = dataset\n",
    "print(wbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=263x229 at 0x1E2059E6C90>, 'label': 4}\n"
     ]
    }
   ],
   "source": [
    "print(wbc['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 301 photos\n",
      "Label 1: 1066 photos\n",
      "Label 2: 3609 photos\n",
      "Label 3: 795 photos\n",
      "Label 4: 10862 photos\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to count the number of photos per label\n",
    "label_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through the 'train' split of your dataset\n",
    "for item in wbc['train']:\n",
    "    # Increment the count for the item's label\n",
    "    label_counts[item['label']] += 1\n",
    "\n",
    "# Print the count of photos for each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} photos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Basophil','Eosinophil','Lymphocyte','Monocyte','Neutrophil']\n",
    "id2label = {id: label for id, label in enumerate(label_names)}\n",
    "label2id = {label: id for id, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutrophil\n"
     ]
    }
   ],
   "source": [
    "print(id2label[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88571b5087334e42b5a146485bdee779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc3c6fdb63542b59591239570648ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855421b3d1f145fb96c5bc53f75337b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/84 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebc66c9f472439383e95999bbfee5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b0e0c4a9e9439594b7d9d8af73ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/84 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Boom4869/Original_WBC/commit/aaa5714135472764bae6e6ef8fcef13218bde6a4', commit_message='Upload dataset', commit_description='', oid='aaa5714135472764bae6e6ef8fcef13218bde6a4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbc.push_to_hub(\"Boom4869/Original_WBC\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "QRYBF4cDndRv",
    "outputId": "1185db71-b9ca-4db5-bb68-25d80b0312df"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wbc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f40aab22f2e5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_test_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wbc' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_split = wbc['train'].train_test_split(test_size=0.3)\n",
    "wbc['train'] = train_test_split['train']\n",
    "wbc['test'] = train_test_split['test']\n",
    "\n",
    "validation_test_split = wbc['test'].train_test_split(test_size=0.5)\n",
    "wbc['val'] = validation_test_split['train']\n",
    "wbc['test'] = validation_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubCK2iwgZBSb"
   },
   "outputs": [],
   "source": [
    "label_names = ['Neutrophil', 'Lymphocyte', 'Monocyte', 'Eosinophil', 'Basophil']\n",
    "id2label = {id: label for id, label in enumerate(label_names)}\n",
    "label2id = {label: id for id, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3de2e48de80642d89f95977dfea3538d",
      "efe9c6571cc5422b8b147e5d6931fa99",
      "2decf08caefb4fde86cc2cf32cd347db",
      "53a761c52944428c958812d39bbc4780",
      "6635d7c80d0842cfbff77187011f0972",
      "d3a50cf98bb144acbdb2ba06ee03b261",
      "8eea27311d65403895e2039dc451a4dd",
      "767edc6507db408591bbfc26a7ccce41",
      "2bafd4d4d815427e89c4dc76bbc76578",
      "deead6ea062f4da5a3d2fd89af9dfcda",
      "7b8e751dd4634e5893f6ea52c2bbd645",
      "a5e6b63c39ae46da95baa050c4206333",
      "578513f558024bdba57aa00de3c68fe8",
      "b8f1576e5f1b4433b2d4431bf80c1444",
      "05a07c5489144486b6f7f54efd6bf6e6",
      "1851efa0f89349b298b956d977506551",
      "31fd11f94c824afc80ab523b8862e7d9",
      "e99103ccb6c44421a4260ff5292f0e06",
      "7bf56089826a48f18560da55d1805b36",
      "ef86a5653e034a7398094f98b9bc4c23",
      "9d294bb76f6644279c3b9489067f74fe",
      "0437ce9918064e0c95cbf6c3be403d78"
     ]
    },
    "id": "PXYYfttfZXGR",
    "outputId": "4ccb1c8e-f0cf-4eec-a406-e7a546a2538b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de2e48de80642d89f95977dfea3538d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e6b63c39ae46da95baa050c4206333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMkFJY_LaDnq"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nBEAeHkaIN7"
   },
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ttWcAWnaJBn"
   },
   "outputs": [],
   "source": [
    "wbc = wbc.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbJs2CkiaS5B"
   },
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAnExGcVaUCs"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1t-8ZRreac38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DTvOT7bad3k",
    "outputId": "15d9dfab-e160-4ce1-f7a9-46c71c9a13bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels= 5,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "mbwqdCXCbE9h",
    "outputId": "8f7eecaa-7c69-43fb-db6f-eaae9e019d9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 01:54, Epoch 12/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.884197</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.628194</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.426221</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.191200</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.168453</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.092500</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.218700</td>\n",
       "      <td>1.043836</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.218700</td>\n",
       "      <td>1.033991</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.218700</td>\n",
       "      <td>0.866847</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.218700</td>\n",
       "      <td>0.954729</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-1 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-2 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-3 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-5 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-6 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-7 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-8 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-10 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-11 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-12 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-13 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-15 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory my_awesome_wbc_model/checkpoint-16 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=0.9962332993745804, metrics={'train_runtime': 116.5509, 'train_samples_per_second': 10.982, 'train_steps_per_second': 0.137, 'total_flos': 7.935393098052403e+16, 'train_loss': 0.9962332993745804, 'epoch': 12.8})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wbc_model_Original\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=16,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=wbc[\"train\"],\n",
    "    eval_dataset=wbc[\"val\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_8F7Ip4J4nl"
   },
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(wbc[\"test\"])\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
